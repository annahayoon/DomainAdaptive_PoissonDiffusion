#!/bin/bash
#SBATCH --job-name=test-photography-resume
#SBATCH --account=fc_oscnn  # Replace with your actual FCA account
#SBATCH --partition=savio3_gpu
#SBATCH --qos=a40_gpu3_normal
#SBATCH --nodes=1
#SBATCH --ntasks=1  # Single GPU test
#SBATCH --cpus-per-task=8
#SBATCH --mem=64G
#SBATCH --gres=gpu:A40:1  # Single A40 GPU
#SBATCH --time=2:00:00  # Short test run
#SBATCH --mail-user=anna_yoon@berkeley.edu
#SBATCH --mail-type=ALL
#SBATCH --output=test_resume_%j.out
#SBATCH --error=test_resume_%j.err

set -euo pipefail

echo "üß™ SINGLE GPU RESUME TEST (Enhanced)"
echo "===================================="
echo "This test diagnoses checkpoint compatibility issues"
echo "and applies the correct fixes for model wrapper issues."
echo ""

# Basic setup
CODE_DIR=${CODE_DIR:-"/global/home/users/anna_yoon/DAPGD"}
cd "$CODE_DIR"

# Load modules (simplified)
if ! command -v module &> /dev/null; then
    if [ -f /etc/profile.d/modules.sh ]; then
        source /etc/profile.d/modules.sh
    fi
fi

module load python/3.9 2>/dev/null || echo "‚ö†Ô∏è Could not load python module"
module load cuda/11.8 2>/dev/null || echo "‚ö†Ô∏è Could not load cuda module"

# Activate virtual environment
if [ -d "venv" ]; then
    source venv/bin/activate
    echo "‚úÖ Virtual environment activated"
else
    echo "‚ùå Virtual environment not found"
    exit 1
fi

# Set checkpoint path
CHECKPOINT_PATH="/global/scratch/users/anna_yoon/data/preprocessed/results/photography_optimized_20250922_041700/checkpoints/checkpoint_step_00050000.pt"

echo "Testing checkpoint: $CHECKPOINT_PATH"

# Test 1: Can we load the checkpoint?
echo ""
echo "üîç TEST 1: Basic checkpoint loading and analysis"
echo "=============================================="

if python -c "
import torch
try:
    checkpoint = torch.load('$CHECKPOINT_PATH', map_location='cpu', weights_only=False)
    print('‚úÖ Checkpoint loads successfully')

    # Check contents
    keys = list(checkpoint.keys())
    print(f'   Checkpoint keys: {keys}')

    if 'model_state_dict' in checkpoint:
        model_keys = list(checkpoint['model_state_dict'].keys())
        print(f'   Model has {len(model_keys)} parameters')

        # Check for DDP keys
        ddp_keys = [k for k in model_keys if k.startswith('module.')]
        model_wrapper_keys = [k for k in model_keys if k.startswith('model.')]

        if ddp_keys:
            print(f'   ‚ö†Ô∏è Found {len(ddp_keys)} DDP keys (module. prefix)')
            print(f'   First few DDP keys: {ddp_keys[:3]}')
        else:
            print('   ‚úÖ No DDP keys found')

        if model_wrapper_keys:
            print(f'   ‚ÑπÔ∏è Found {len(model_wrapper_keys)} model wrapper keys (model. prefix)')
        else:
            print('   ‚ÑπÔ∏è No model wrapper keys found')

        # Show sample keys
        print(f'   Sample model keys: {model_keys[:3]}')

    if 'step' in checkpoint:
        print(f'   Training step: {checkpoint[\"step\"]}')
    if 'epoch' in checkpoint:
        print(f'   Training epoch: {checkpoint[\"epoch\"]}')

except Exception as e:
    print(f'‚ùå Failed to load checkpoint: {e}')
    exit(1)
"; then
    echo "‚úÖ Test 1 passed"
else
    echo "‚ùå Test 1 failed"
    exit 1
fi

# Test 2: Fix checkpoint issues (DDP + model wrapper)
echo ""
echo "üîß TEST 2: Comprehensive checkpoint fixing"
echo "========================================="

# First, fix DDP keys with the existing script
FIXED_CHECKPOINT_DDP="${CHECKPOINT_PATH%.*}_fixed_ddp.${CHECKPOINT_PATH##*.}"

echo "Step 1: Fixing DDP keys..."
if python fix_ddp_checkpoint.py "$CHECKPOINT_PATH" --output "$FIXED_CHECKPOINT_DDP"; then
    echo "‚úÖ DDP keys fixed successfully"
else
    echo "‚ùå DDP key fixing failed"
    exit 1
fi

# Step 2: Add model wrapper prefix for gradient checkpointing compatibility
FIXED_CHECKPOINT="${CHECKPOINT_PATH%.*}_fixed_enhanced.${CHECKPOINT_PATH##*.}"

echo "Step 2: Adding model wrapper prefix for gradient checkpointing..."
if python -c "
import torch

# Load the DDP-fixed checkpoint
checkpoint = torch.load('$FIXED_CHECKPOINT_DDP', map_location='cpu', weights_only=False)
print('‚úÖ Loaded DDP-fixed checkpoint')

# Fix the model state dict by adding model. prefix
model_state = checkpoint['model_state_dict']
fixed_model_state = {}

for key, value in model_state.items():
    if not key.startswith('model.'):
        new_key = f'model.{key}'
        fixed_model_state[new_key] = value
    else:
        fixed_model_state[key] = value

checkpoint['model_state_dict'] = fixed_model_state
print(f'‚úÖ Added model. prefix to {len(model_state)} parameters')

# Also fix EMA model if present
if 'ema_model_state_dict' in checkpoint:
    ema_state = checkpoint['ema_model_state_dict']
    fixed_ema_state = {}

    for key, value in ema_state.items():
        if not key.startswith('model.'):
            new_key = f'model.{key}'
            fixed_ema_state[new_key] = value
        else:
            fixed_ema_state[key] = value

    checkpoint['ema_model_state_dict'] = fixed_ema_state
    print('‚úÖ Also fixed EMA model state dict')

# Fix RNG state compatibility issues (PyTorch version differences)
if 'rng_state' in checkpoint and checkpoint['rng_state'] is not None:
    rng_state = checkpoint['rng_state']
    if not isinstance(rng_state, torch.ByteTensor):
        if hasattr(rng_state, 'byte'):
            checkpoint['rng_state'] = rng_state.byte()
        else:
            # Remove incompatible RNG state
            del checkpoint['rng_state']
        print('üîß Fixed RNG state compatibility')

if 'cuda_rng_state' in checkpoint and checkpoint['cuda_rng_state'] is not None:
    cuda_rng_state = checkpoint['cuda_rng_state']
    if not isinstance(cuda_rng_state, torch.ByteTensor):
        if hasattr(cuda_rng_state, 'byte'):
            checkpoint['cuda_rng_state'] = cuda_rng_state.byte()
        else:
            # Remove incompatible CUDA RNG state
            del checkpoint['cuda_rng_state']
        print('üîß Fixed CUDA RNG state compatibility')

# Save the fully fixed checkpoint
torch.save(checkpoint, '$FIXED_CHECKPOINT')
print('‚úÖ Saved comprehensive fixed checkpoint')
"; then
    echo "‚úÖ Model wrapper prefix added successfully"
    CHECKPOINT_PATH="$FIXED_CHECKPOINT"

    # Clean up intermediate file
    rm -f "$FIXED_CHECKPOINT_DDP"
else
    echo "‚ùå Model wrapper prefix addition failed"
    exit 1
fi

# Test 3: Can we create a model and load the fixed checkpoint?
echo ""
echo "ü§ñ TEST 3: Model creation and fixed checkpoint loading"
echo "===================================================="

# Set basic environment for single GPU
export CUDA_VISIBLE_DEVICES=0
unset RANK WORLD_SIZE LOCAL_RANK

# Test model creation and checkpoint loading
if python -c "
import sys
sys.path.insert(0, '.')

import torch
from train_photography_model import PhotographyTrainingManager

print('Creating training manager...')
# Create training manager
manager = PhotographyTrainingManager(
    data_root='/global/scratch/users/anna_yoon/data/preprocessed',
    output_dir='test_resume_output',
    device='cuda',
    seed=42
)

print('‚úÖ Training manager created')

print('Creating model with gradient checkpointing...')
# Create model with same config as original training
model = manager.create_model(
    use_multi_resolution=False,
    mixed_precision=True,
    gradient_checkpointing=True,  # This adds the model. wrapper
    model_channels=256,
    channel_mult=[1, 2, 3, 4],
    channel_mult_emb=6,
    num_blocks=6,
    attn_resolutions=[16, 32, 64]
)

print('‚úÖ Model created successfully')
print(f'   Model device: {next(model.parameters()).device}')
print(f'   Model parameters: {sum(p.numel() for p in model.parameters()):,}')

# Show the model structure to understand the key format
print('üîç Analyzing model parameter keys...')
model_keys = list(model.state_dict().keys())
print(f'   Total model keys: {len(model_keys)}')
print(f'   Sample model keys: {model_keys[:3]}')

# Check if model has the wrapper prefix
has_model_wrapper = any(key.startswith('model.') for key in model_keys)
print(f'   Model has wrapper prefix: {has_model_wrapper}')

# Try to load checkpoint
print('üîÑ Loading fixed checkpoint into model...')
try:
    checkpoint = torch.load('$CHECKPOINT_PATH', map_location='cuda', weights_only=False)

    # Show checkpoint keys for comparison
    ckpt_keys = list(checkpoint['model_state_dict'].keys())
    print(f'   Checkpoint keys: {len(ckpt_keys)}')
    print(f'   Sample checkpoint keys: {ckpt_keys[:3]}')

    # Load the state dict
    model.load_state_dict(checkpoint['model_state_dict'])
    print('‚úÖ Checkpoint loaded successfully into model')

    # Verify model is in correct state
    model.eval()
    with torch.no_grad():
        # Test forward pass with dummy data
        dummy_input = torch.randn(1, 4, 128, 128).cuda()
        dummy_sigma = torch.tensor([1.0]).cuda()

        try:
            output = model(dummy_input, dummy_sigma)
            print(f'‚úÖ Forward pass successful, output shape: {output.shape}')
        except Exception as e:
            print(f'‚ö†Ô∏è Forward pass failed: {e}')

except Exception as e:
    print(f'‚ùå Failed to load checkpoint into model: {e}')
    import traceback
    traceback.print_exc()
    exit(1)

print('üéâ All model tests passed! Checkpoint is compatible with gradient checkpointing.')
"; then
    echo "‚úÖ Test 3 passed"
else
    echo "‚ùå Test 3 failed"
    exit 1
fi

# Test 4: Short training run
echo ""
echo "üèÉ TEST 4: Short training test with fixed checkpoint"
echo "=================================================="

echo "Attempting a very short training run to verify everything works..."

# Run a minimal training test
if python train_photography_model.py \
    --data_root "/global/scratch/users/anna_yoon/data/preprocessed" \
    --resume_checkpoint "$CHECKPOINT_PATH" \
    --max_steps 10 \
    --batch_size 1 \
    --learning_rate 1e-6 \
    --model_channels 256 \
    --channel_mult 1 2 3 4 \
    --channel_mult_emb 6 \
    --num_blocks 6 \
    --attn_resolutions 16 32 64 \
    --output_dir "test_resume_output" \
    --device cuda \
    --mixed_precision true \
    --gradient_checkpointing true \
    --num_workers 0 \
    --save_frequency_steps 5 \
    --val_frequency_steps 10; then
    echo "‚úÖ Test 4 passed - Short training run successful"
else
    echo "‚ùå Test 4 failed - Training run failed"
    exit 1
fi

echo ""
echo "üéâ SUCCESS: All enhanced tests passed!"
echo "===================================="
echo "The checkpoint has been successfully fixed and is compatible."
echo ""
echo "Key findings:"
echo "1. ‚úÖ Original checkpoint had DDP keys that were removed"
echo "2. ‚úÖ Added model wrapper prefix for gradient checkpointing compatibility"
echo "3. ‚úÖ Model can load the fixed checkpoint successfully"
echo "4. ‚úÖ Forward pass and training work correctly"
echo ""
echo "The issue was a combination of:"
echo "- DDP key prefixes from distributed training"
echo "- Missing model wrapper prefix for gradient checkpointing"
echo ""
echo "Fixed checkpoint saved as: $CHECKPOINT_PATH"

# Clean up test files
echo ""
echo "üßπ Cleaning up test files..."
if [ -f "${CHECKPOINT_PATH%.*}_fixed_test.${CHECKPOINT_PATH##*.}" ]; then
    rm "${CHECKPOINT_PATH%.*}_fixed_test.${CHECKPOINT_PATH##*.}"
fi

echo "‚úÖ Test completed successfully!"
