#!/bin/bash

# H100-OPTIMIZED PRIOR_CLEAN UNIFIED TRAINING CONFIGURATION
# Maximizes H100 performance for the 42K+ prior_clean dataset
# This is the OPTIMAL configuration for your research

echo "ðŸš€ H100-OPTIMIZED PRIOR_CLEAN UNIFIED TRAINING v1.0"
echo "=================================================================="
echo ""
echo "Strategy: MAXIMUM H100 performance for 42K+ prior_clean dataset"
echo "âœ… DATASET: 42,109 clean tiles (128x128) across 3 domains"
echo "âœ… OPTIMIZED: Large batches leveraging 80GB HBM3"
echo "âœ… UNIFIED: Single model with domain conditioning"
echo "âœ… BALANCED: Weighted sampling across all domains"
echo "âœ… RESEARCH: Perfect for diffusion prior training"
echo ""

# Kill existing sessions
tmux kill-session -t h100_prior_clean_training 2>/dev/null

# Create new session
tmux new-session -d -s h100_prior_clean_training -c /opt/dlami/nvme/DomainAdaptive_PoissonDiffusion

# H100-optimized configuration for prior_clean
MIXED_PRECISION_FLAG="true"
PRECISION_MODE="bf16"
LEARNING_RATE="1e-4"        # Higher rate for large batches
BATCH_SIZE="2"              # Reduced batch size to prevent OOM with 1.6B model
GRAD_ACCUM="32"             # Effective batch = 64 (maintained)
MODEL_CHANNELS="320"        # Large model for H100
NUM_BLOCKS="8"              # Deep model
MAX_STEPS="450000"          # Optimized for faster completion (1.5-2 days)

# Check for existing checkpoint - look in the correct H100 training directory
STABLE_CHECKPOINT=""
LATEST_CHECKPOINT=""

# Find the most recent checkpoint in the H100 training directory
if [ -d "results/prior_clean_h100_training" ]; then
    LATEST_CHECKPOINT=$(ls -t results/prior_clean_h100_training/checkpoint_step_*.pth 2>/dev/null | head -1)
    if [ -n "$LATEST_CHECKPOINT" ]; then
        STABLE_CHECKPOINT="--resume_checkpoint $LATEST_CHECKPOINT"
        echo "ðŸ“Š Resuming from latest checkpoint: $(basename $LATEST_CHECKPOINT)"
    else
        # Check for best model if no step checkpoints
        if [ -f "results/prior_clean_h100_training/best_model.pth" ]; then
            STABLE_CHECKPOINT="--resume_checkpoint results/prior_clean_h100_training/best_model.pth"
            echo "ðŸ“Š Resuming from best model checkpoint"
        else
            echo "ðŸ“Š Starting fresh training"
        fi
    fi
else
    echo "ðŸ“Š Starting fresh training"
fi

# H100-optimized configuration summary
echo "ðŸš€ H100-OPTIMIZED PRIOR_CLEAN CONFIGURATION:"
echo "  Dataset: 42,109 tiles (30K train, 5.6K val, 6.4K test)"
echo "  Domains: Photography (4ch), Microscopy (1ch), Astronomy (1ch)"
echo "  Model: ${MODEL_CHANNELS}ch, ${NUM_BLOCKS} blocks, H100-optimized"
echo "  Training: ${MAX_STEPS} steps (optimized for faster completion)"
echo "  Batch Strategy: ${BATCH_SIZE} physical Ã— ${GRAD_ACCUM} accumulation = $((BATCH_SIZE * GRAD_ACCUM)) effective"
echo "  H100 Features: BF16, TF32, Flash Attention, 80GB HBM3"
echo "  Learning Rate: $LEARNING_RATE (optimized for large batches)"
echo "  Mixed Precision: $MIXED_PRECISION_FLAG (BF16 on H100)"

# H100-specific environment optimizations with better memory management
export PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True,max_split_size_mb:256,garbage_collection_threshold:0.8"
export CUDA_LAUNCH_BLOCKING=0
export TORCH_CUDNN_V8_API_ENABLED=1
export TORCH_CUDNN_ALLOW_TF32=1
export TORCH_ALLOW_TF32_CUBLAS_OVERRIDE=1
export OMP_NUM_THREADS=16
# Clear cache before starting
export PYTORCH_NO_CUDA_MEMORY_CACHING=0

# Launch H100-optimized prior_clean training
tmux send-keys -t h100_prior_clean_training "
cd /opt/dlami/nvme/DomainAdaptive_PoissonDiffusion && \\
source .venv/bin/activate && \\
python train_unified_prior_clean.py \\
    --data_root /opt/dlami/nvme/preprocessed/prior_clean \\
    --domains photography microscopy astronomy \\
    --max_steps ${MAX_STEPS} \\
    --batch_size ${BATCH_SIZE} \\
    --gradient_accumulation_steps ${GRAD_ACCUM} \\
    --learning_rate $LEARNING_RATE \\
    --model_channels ${MODEL_CHANNELS} \\
    --num_blocks ${NUM_BLOCKS} \\
    --channel_mult_emb 8 \\
    --output_dir results/prior_clean_h100_training \\
    --device cuda \\
    --h100_optimizations \\
    --mixed_precision \\
    --num_workers 8 \\
    --seed 42 \\
    $STABLE_CHECKPOINT
" C-m

echo ""
echo "ðŸš€ H100-OPTIMIZED PRIOR_CLEAN TRAINING STARTED!"
echo ""
echo "ðŸ“Š H100 Configuration:"
echo "  â€¢ Dataset: 42,109 clean tiles (perfect for diffusion prior)"
echo "  â€¢ Model: ${MODEL_CHANNELS}-channel H100-optimized architecture"
echo "  â€¢ Training: ${MAX_STEPS} steps (research-quality convergence)"
echo "  â€¢ Batch Size: ${BATCH_SIZE} (leveraging 80GB HBM3)"
echo "  â€¢ Effective Batch: $((BATCH_SIZE * GRAD_ACCUM)) (${BATCH_SIZE}Ã—${GRAD_ACCUM} accumulation)"
echo "  â€¢ Precision: BF16 (H100 native support)"
echo "  â€¢ Learning Rate: $LEARNING_RATE (large batch optimized)"
echo ""
echo "ðŸ”¥ H100-Specific Features:"
echo "  â€¢ 80GB HBM3 memory fully utilized"
echo "  â€¢ BF16 mixed precision (H100 optimized)"
echo "  â€¢ TF32 enabled for 2x matrix speedup"
echo "  â€¢ Optimized CUDA memory allocation"
echo "  â€¢ Multi-domain balanced sampling"
echo "  â€¢ Domain conditioning vectors"
echo ""
echo "ðŸ“ˆ Dataset Advantages:"
echo "  â€¢ Pre-tiled to 128Ã—128 (perfect size)"
echo "  â€¢ Clean images only (ideal for prior training)"
echo "  â€¢ 3 domains with proper balance"
echo "  â€¢ 42K+ samples for robust training"
echo ""
echo "ðŸ“± Monitor with:"
echo "  tmux attach -t h100_prior_clean_training"
echo ""
echo "ðŸŽ¯ Expected Performance:"
echo "   Training time: ~1.5-2 days for 450K steps"
echo "   Memory usage: ~70-75GB (optimal H100 utilization)"
echo "   Speed: ~2-3 steps/second"
echo "   Quality: Research-grade unified model"
echo ""
echo "ðŸ“‹ Phase Checkpointing:"
echo "   Step 100K: Photography-only best model"
echo "   Step 200K: Photography+Microscopy best model"
echo "   Step 300K: All domains phase 1"
echo "   Step 400K: All domains phase 2"
echo "   Step 450K: Final unified model"
echo ""
echo "ðŸ’¾ Checkpoints saved every 25K steps to:"
echo "   results/prior_clean_h100_training/checkpoints/"
