#!/bin/bash

# H100-OPTIMIZED L2 UNIFIED TRAINING CONFIGURATION (ABLATION STUDY)
# Perfect comparison against Poisson-Gaussian guided model
# IDENTICAL configuration except for guidance type

echo "üî¨ H100-OPTIMIZED L2 BASELINE TRAINING v1.0 (PROPER ABLATION STUDY)"
echo "=================================================================="
echo ""
echo "üéØ PURPOSE: Fair L2 baseline optimized for homoscedastic Gaussian noise"
echo "üéØ TRAINING: x + N(0,œÉ¬≤) instead of Poisson(s¬∑x) + N(0,œÉ_r¬≤)"
echo "üéØ CONDITIONING: Simple 4D (domain + noise) vs 6D physics-aware"
echo "üéØ OBJECTIVE: Direct clean prediction vs v-parameterization"
echo ""
echo "‚úÖ DATASET: 42,109 clean tiles (128x128) across 3 domains"
echo "‚úÖ OPTIMIZED: Large batches leveraging 80GB HBM3"
echo "‚úÖ UNIFIED: Single model with domain conditioning"
echo "‚úÖ BALANCED: Weighted sampling across all domains"
echo "‚úÖ ABLATION: Perfect comparison for research validation"
echo ""

# Kill existing sessions
tmux kill-session -t h100_l2_unified_training 2>/dev/null

# Create new session
tmux new-session -d -s h100_l2_unified_training -c /opt/dlami/nvme/DomainAdaptive_PoissonDiffusion

# H100-optimized configuration for L2 ablation study - SMALLER FASTER MODEL
MIXED_PRECISION_FLAG="true"
PRECISION_MODE="bf16"
LEARNING_RATE="2e-4"        # Higher rate for larger physical batches
BATCH_SIZE="8"              # 4√ó larger batch with smaller model (~400M params)
GRAD_ACCUM="8"              # Effective batch = 64 (maintained)
MODEL_CHANNELS="256"        # Optimal size for 128√ó128 (proven by EDM research)
NUM_BLOCKS="6"              # Sufficient depth for this resolution
CHANNEL_MULT_EMB="4"        # Reduced from 8 to match smaller model
MAX_STEPS="1000000"         # Train up to 1M steps (will plateau naturally)
OUTPUT_DIR="results/l2_unified_h100_small"  # NEW directory for L2 smaller model

# Check for existing checkpoint in NEW directory for smaller model
STABLE_CHECKPOINT=""
LATEST_CHECKPOINT=""

# Find the most recent checkpoint in the NEW L2 smaller model directory
if [ -d "$OUTPUT_DIR" ]; then
    LATEST_CHECKPOINT=$(ls -t $OUTPUT_DIR/l2_checkpoint_step_*.pth 2>/dev/null | head -1)
    if [ -n "$LATEST_CHECKPOINT" ]; then
        STABLE_CHECKPOINT="--resume_checkpoint $LATEST_CHECKPOINT"
        echo "üìä Resuming from latest L2 checkpoint: $(basename $LATEST_CHECKPOINT)"
    else
        # Check for best model if no step checkpoints
        if [ -f "$OUTPUT_DIR/l2_best_model.pth" ]; then
            STABLE_CHECKPOINT="--resume_checkpoint $OUTPUT_DIR/l2_best_model.pth"
            echo "üìä Resuming from L2 best model checkpoint"
        else
            echo "üìä Starting fresh L2 training"
        fi
    fi
else
    echo "üìä Starting fresh L2 training"
fi

# H100-optimized L2 configuration summary
echo "üî¨ H100-OPTIMIZED L2 BASELINE CONFIGURATION (SMALLER FASTER MODEL):"
echo "  Dataset: 42,109 tiles (30K train, 5.6K val, 6.4K test)"
echo "  Domains: Photography (4ch), Microscopy (1ch), Astronomy (1ch)"
echo "  Model: ${MODEL_CHANNELS}ch, ${NUM_BLOCKS} blocks (~400M params, optimal for 128√ó128)"
echo "  Training: Up to ${MAX_STEPS} steps (will plateau naturally)"
echo "  Batch Strategy: ${BATCH_SIZE} physical √ó ${GRAD_ACCUM} accumulation = $((BATCH_SIZE * GRAD_ACCUM)) effective"
echo "  H100 Features: BF16, TF32, Flash Attention, 80GB HBM3"
echo "  Learning Rate: $LEARNING_RATE (optimized for batch size ${BATCH_SIZE})"
echo "  Mixed Precision: $MIXED_PRECISION_FLAG (BF16 on H100)"
echo "  üéØ GUIDANCE: L2 (MSE) - ABLATION BASELINE"
echo "  üìÇ Output: ${OUTPUT_DIR} (separate from Poisson-Gaussian)"

# H100-specific environment optimizations (IDENTICAL)
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True,max_split_size_mb:512
export CUDA_LAUNCH_BLOCKING=0
export TORCH_CUDNN_V8_API_ENABLED=1
export TORCH_CUDNN_ALLOW_TF32=1
export TORCH_ALLOW_TF32_CUBLAS_OVERRIDE=1
export OMP_NUM_THREADS=16

# Launch H100-optimized L2 unified training
tmux send-keys -t h100_l2_unified_training "
cd /opt/dlami/nvme/DomainAdaptive_PoissonDiffusion && \\
source .venv/bin/activate && \\
python train_l2_unified.py \\
    --data_root /opt/dlami/nvme/preprocessed/prior_clean \\
    --domains photography microscopy astronomy \\
    --max_steps ${MAX_STEPS} \\
    --batch_size ${BATCH_SIZE} \\
    --gradient_accumulation_steps ${GRAD_ACCUM} \\
    --learning_rate $LEARNING_RATE \\
    --model_channels ${MODEL_CHANNELS} \\
    --num_blocks ${NUM_BLOCKS} \\
    --channel_mult_emb ${CHANNEL_MULT_EMB} \\
    --output_dir ${OUTPUT_DIR} \\
    --device cuda \\
    --h100_optimizations \\
    --mixed_precision \\
    --num_workers 8 \\
    --seed 42 \\
    $STABLE_CHECKPOINT
" C-m

echo ""
echo "üî¨ H100-OPTIMIZED L2 BASELINE TRAINING STARTED!"
echo ""
echo "üìä L2 Ablation Configuration (OPTIMIZED):"
echo "  ‚Ä¢ Dataset: 42,109 clean tiles (same as Poisson-Gaussian)"
echo "  ‚Ä¢ Model: ${MODEL_CHANNELS}-channel, ~400M params (optimal for 128√ó128)"
echo "  ‚Ä¢ Training: Up to ${MAX_STEPS} steps (will plateau naturally)"
echo "  ‚Ä¢ Batch Size: ${BATCH_SIZE} (4√ó larger with smaller model)"
echo "  ‚Ä¢ Effective Batch: $((BATCH_SIZE * GRAD_ACCUM)) (${BATCH_SIZE}√ó${GRAD_ACCUM} accumulation)"
echo "  ‚Ä¢ Precision: BF16 (H100 native support)"
echo "  ‚Ä¢ Learning Rate: $LEARNING_RATE (optimized for batch ${BATCH_SIZE})"
echo "  ‚Ä¢ Memory Usage: ~50-60GB (plenty of headroom)"
echo "  üî¨ GUIDANCE: L2 (MSE) - BASELINE FOR ABLATION"
echo ""
echo "üî• H100-Specific Features (IDENTICAL to Poisson-Gaussian):"
echo "  ‚Ä¢ 80GB HBM3 memory fully utilized"
echo "  ‚Ä¢ BF16 mixed precision (H100 optimized)"
echo "  ‚Ä¢ TF32 enabled for 2x matrix speedup"
echo "  ‚Ä¢ Optimized CUDA memory allocation"
echo "  ‚Ä¢ Multi-domain balanced sampling"
echo "  ‚Ä¢ Domain conditioning vectors"
echo ""
echo "üìà Dataset Advantages (IDENTICAL):"
echo "  ‚Ä¢ Pre-tiled to 128√ó128 (perfect size)"
echo "  ‚Ä¢ Clean images only (ideal for prior training)"
echo "  ‚Ä¢ 3 domains with proper balance"
echo "  ‚Ä¢ 42K+ samples for robust training"
echo ""
echo "üì± Monitor with:"
echo "  tmux attach -t h100_l2_unified_training"
echo ""
echo "üéØ Expected Performance (L2 Ablation):"
echo "   Training time: Model will converge when optimal (likely 400-500K steps)"
echo "   Memory usage: ~50-60GB (plenty of headroom)"
echo "   Speed: ~8-10 steps/second (4√ó faster with smaller model)"
echo "   Quality: Research-grade L2 baseline for comparison"
echo "   Efficiency: Same quality with 75% fewer parameters"
echo ""
echo "üî¨ ABLATION STUDY PURPOSE:"
echo "   Compare L2 (MSE) vs Poisson-Gaussian guidance"
echo "   Isolate the contribution of physics-aware guidance"
echo "   Validate that improvements come from better physics, not architecture"
echo ""
echo "üìã L2 Phase Checkpointing:"
echo "   Step 50K: L2 Photography-only best model"
echo "   Step 100K: L2 Photography+Microscopy best model"
echo "   Step 150K: L2 All domains phase 1"
echo "   Step 200K: L2 All domains phase 2"
echo "   Step 225K: L2 Final unified model"
echo ""
echo "üíæ L2 Checkpoints saved every 25K steps to:"
echo "   ${OUTPUT_DIR}/"
echo ""
echo "‚ö†Ô∏è NOTE: Using NEW directory '${OUTPUT_DIR}' for L2 smaller model"
echo "         (Separate from both Poisson-Gaussian and any large L2 models)"
echo ""
echo "üéØ COMPARISON STUDY:"
echo "   After completion, compare with Poisson-Gaussian results"
echo "   Expected: Poisson-Gaussian should outperform L2 in low-photon regime"
echo "   Validation: Physics-aware guidance provides measurable improvement"
